{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:04:36.176902Z",
     "start_time": "2024-05-02T01:04:26.214115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data-desc', 'IMDB_dataset.csv', 'RT_IOT2022.csv', 'breast-cancer-wisconsin.csv']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# https://www.kaggle.com/code/sparshnagpal/imdb-review-test-notebook\n",
    "\n",
    "\n",
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../data\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import the traning dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e09469f85bccbdd2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n5  Probably my all-time favorite movie, a story o...  positive\n6  I sure would like to see a resurrection of a u...  positive\n7  This show was an amazing, fresh & innovative i...  negative\n8  Encouraged by the positive comments about this...  negative\n9  If you like original gut wrenching laughter yo...  positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Probably my all-time favorite movie, a story o...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I sure would like to see a resurrection of a u...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This show was an amazing, fresh &amp; innovative i...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Encouraged by the positive comments about this...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>If you like original gut wrenching laughter yo...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('../data/IMDB_dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:04:49.947524Z",
     "start_time": "2024-05-02T01:04:48.713913Z"
    }
   },
   "id": "d4363cf44d6886e2",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exploratery data analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21305cf11447909d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   review sentiment\ncount                                               50000     50000\nunique                                              49582         2\ntop     Loved today's show!!! It was a variety and not...  positive\nfreq                                                    5     25000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50000</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49582</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Loved today's show!!! It was a variety and not...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>25000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset\n",
    "imdb_data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:05:58.187963Z",
     "start_time": "2024-05-02T01:05:58.051593Z"
    }
   },
   "id": "a212e4e010815238",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentiment count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5284c94bc3e4f22d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "sentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment count\n",
    "imdb_data['sentiment'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:18:24.700993Z",
     "start_time": "2024-05-02T01:18:24.692648Z"
    }
   },
   "id": "4c13ade8f57bb009",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shuffle the data randomly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ab63f31b99e970"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_shuffled = imdb_data.sample(frac=1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:25:24.323981Z",
     "start_time": "2024-05-02T01:25:24.314530Z"
    }
   },
   "id": "da059dc12ee5b74b",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spliting the training datasetï¼Œavoid Prevent data contamination"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bac00e76d24e78d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "\n",
    "#test dataset\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:25:50.554385Z",
     "start_time": "2024-05-02T01:25:50.548477Z"
    }
   },
   "id": "185c9ea9f7454fe9",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdf02e54ac1ab6d6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/neverland/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "nltk.download('stopwords')\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:27:48.177649Z",
     "start_time": "2024-05-02T01:27:45.920703Z"
    }
   },
   "id": "16db48d7606ebc69",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing html strips and noise text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "548e558f85349b67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:38:22.733313Z",
     "start_time": "2024-05-02T01:38:13.785280Z"
    }
   },
   "id": "4e3b7ced3474c5af",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing special characters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a983fec47159871"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:39:18.804688Z",
     "start_time": "2024-05-02T01:39:17.016899Z"
    }
   },
   "id": "ae7671e1eab54698",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text stemming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d790366209fea16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:43:09.635265Z",
     "start_time": "2024-05-02T01:40:20.535809Z"
    }
   },
   "id": "95e4349d625c593d",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing stopwords\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b7826fbaaf20db3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am', 'themselves', 'his', 'how', 'down', 'mustn', \"she's\", \"hasn't\", \"shan't\", 'are', 'very', \"you've\", \"you're\", 'the', 'herself', 'when', 'himself', 'few', 'be', 'hasn', 'other', 'than', 'we', 'too', 'nor', \"aren't\", 'been', 'through', 're', 'weren', 'yourselves', 'should', \"you'd\", 'no', 'doesn', 'has', 'only', 'then', 'they', 'both', 'aren', 'own', 'shan', 'some', 'me', 'd', 'each', \"didn't\", 'yours', 'above', \"it's\", 'during', 'its', 'itself', \"doesn't\", \"you'll\", 'ours', 'that', 'which', 'all', 'her', 'here', 'hers', 'just', 'ourselves', 'but', 'ma', 'm', 'on', \"mustn't\", 'there', 'while', 'wasn', \"couldn't\", 'needn', 'couldn', 'because', 'an', 'before', 'of', 'a', 'for', 'don', 'not', 'if', \"wouldn't\", 'out', 'o', 'ain', 'whom', 'again', 'does', 'my', 's', 'about', 'between', 'once', 'theirs', 'until', \"hadn't\", 'having', \"weren't\", 'against', 'he', 'in', 'was', 'most', 'so', 'doing', 't', 'can', 'any', 'you', 'your', 'at', 'now', 'under', 'what', 'didn', 'did', 'wouldn', 'where', \"should've\", 'myself', 'same', \"won't\", 'him', 'off', 'y', 'isn', 'these', 'i', 'up', 'and', 'more', 'll', 'mightn', 'won', 'below', 'yourself', 'from', \"isn't\", 'had', \"wasn't\", \"that'll\", 'their', 'our', 'such', 'haven', \"mightn't\", 'were', 'shouldn', 'why', \"don't\", 'it', 'or', 'to', 'further', 'she', \"haven't\", 'into', 'do', 'will', \"needn't\", 'with', 'this', 'those', 'is', 'them', 'who', 'after', 'by', 'being', 've', 'over', 'have', \"shouldn't\", 'as', 'hadn'}\n"
     ]
    }
   ],
   "source": [
    "#set stopwords to english\n",
    "# Removing stopwords can help in reducing the dimensionality of the dataset and improving the performance of tasks such as text classification, information retrieval, and sentiment analysis, as it allows the model to focus on the more meaningful words in the text. However, the list of stopwords can vary depending on the context and language being analyzed.\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T01:49:34.091792Z",
     "start_time": "2024-05-02T01:49:00.139493Z"
    }
   },
   "id": "47e3e968f9e38ce6",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalized train reviews"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8920df64787c6269"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/textblob/decorators.py:35\u001B[0m, in \u001B[0;36mrequires_nltk_corpus.<locals>.decorated\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/textblob/tokenizers.py:59\u001B[0m, in \u001B[0;36mSentenceTokenizer.tokenize\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return a list of sentences.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03musing NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m:param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 106\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizers/punkt/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlanguage\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pickle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mtokenize(text)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/nltk/data.py:750\u001B[0m, in \u001B[0;36mload\u001B[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;66;03m# Load the resource.\u001B[39;00m\n\u001B[0;32m--> 750\u001B[0m opened_resource \u001B[38;5;241m=\u001B[39m \u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresource_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/nltk/data.py:876\u001B[0m, in \u001B[0;36m_open\u001B[0;34m(resource_url)\u001B[0m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m protocol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnltk\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 876\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mopen()\n\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    878\u001B[0m     \u001B[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/nltk/data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/PY3/english.pickle\u001B[0m\n\n  Searched in:\n    - '/Users/neverland/nltk_data'\n    - '/opt/anaconda3/envs/ml/nltk_data'\n    - '/opt/anaconda3/envs/ml/share/nltk_data'\n    - '/opt/anaconda3/envs/ml/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mMissingCorpusError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m norm_train_spelling\u001B[38;5;241m.\u001B[39mcorrect()\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#Tokenization using Textblob\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m norm_train_words\u001B[38;5;241m=\u001B[39m\u001B[43mnorm_train_spelling\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m\n\u001B[1;32m     11\u001B[0m norm_train_words\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/textblob/decorators.py:23\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, cls)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[0;32m---> 23\u001B[0m value \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m value\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/textblob/blob.py:619\u001B[0m, in \u001B[0;36mTextBlob.words\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    611\u001B[0m \u001B[38;5;129m@cached_property\u001B[39m\n\u001B[1;32m    612\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwords\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    613\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a list of word tokens. This excludes punctuation characters.\u001B[39;00m\n\u001B[1;32m    614\u001B[0m \u001B[38;5;124;03m    If you want to include punctuation characters, access the ``tokens``\u001B[39;00m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;124;03m    property.\u001B[39;00m\n\u001B[1;32m    616\u001B[0m \n\u001B[1;32m    617\u001B[0m \u001B[38;5;124;03m    :returns: A :class:`WordList <WordList>` of word tokens.\u001B[39;00m\n\u001B[1;32m    618\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 619\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m WordList(\u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude_punc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/textblob/tokenizers.py:76\u001B[0m, in \u001B[0;36mword_tokenize\u001B[0;34m(text, include_punc, *args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mword_tokenize\u001B[39m(text, include_punc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     69\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Convenience function for tokenizing text into words.\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    NOTE: NLTK's word tokenizer expects sentences as input, so the text will be\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;03m    tokenized to sentences before being tokenized to words.\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     74\u001B[0m     words \u001B[38;5;241m=\u001B[39m chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[1;32m     75\u001B[0m         _word_tokenizer\u001B[38;5;241m.\u001B[39mitokenize(sentence, include_punc, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 76\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     )\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m words\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/textblob/base.py:67\u001B[0m, in \u001B[0;36mBaseTokenizer.itokenize\u001B[0;34m(self, text, *args, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mitokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     61\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a generator that generates tokens \"on-demand\".\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \n\u001B[1;32m     63\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 0.6.0\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :rtype: generator\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (t \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/textblob/decorators.py:37\u001B[0m, in \u001B[0;36mrequires_nltk_corpus.<locals>.decorated\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m---> 37\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MissingCorpusError() \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m\n",
      "\u001B[0;31mMissingCorpusError\u001B[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "#normalized train reviews\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_train_reviews[0]\n",
    "#convert dataframe to string\n",
    "norm_train_string=norm_train_reviews.to_string()\n",
    "#Spelling correction using Textblob\n",
    "norm_train_spelling=TextBlob(norm_train_string)\n",
    "norm_train_spelling.correct()\n",
    "#Tokenization using Textblob\n",
    "norm_train_words=norm_train_spelling.words\n",
    "norm_train_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T04:33:32.056327Z",
     "start_time": "2024-05-02T02:19:29.297427Z"
    }
   },
   "id": "3bbd8619a81852d6",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalized test reviews.\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a simple API for common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58a259f98691486e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Normalized test reviews\n",
    "norm_test_reviews=imdb_data.review[40000:]\n",
    "norm_test_reviews[45005]\n",
    "##convert dataframe to string\n",
    "norm_test_string=norm_test_reviews.to_string()\n",
    "#spelling correction using Textblob\n",
    "norm_test_spelling=TextBlob(norm_test_string)\n",
    "print(norm_test_spelling.correct())\n",
    "#Tokenization using Textblob\n",
    "norm_test_words=norm_test_spelling.words\n",
    "norm_test_words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72fb4318d4698e45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bags of words model\n",
    "It is used to convert text documents to numerical vectors or bag of words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6aec5ff990a9936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "288fc90129a32f00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Labeling the sentiment text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "468d0516e1ba6251"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T04:52:11.264446Z",
     "start_time": "2024-05-02T04:52:11.127357Z"
    }
   },
   "id": "28d224f3d7f9091b",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the sentiment tdata"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfeb7d50dd105d88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c9019576f7d3f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modelling the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44e99b607ddeb48"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
    "print(lr_tfidf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3ce0ff1896a2fe0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic regression model performane on test dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f74d1f9d3d2f85"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e2557cdd63ddda4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c2d098f8cfba1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
    "print(\"lr_bow_score :\",lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",lr_tfidf_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a803d02fac355b07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the classification report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68c9925d0f2bca6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(lr_tfidf_report)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ddd06fa77b5b92b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "621bddb2a0c07ec9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45a4bb013c40c667"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stochastic gradient descent or Linear support vector machines for bag of words and tfidf features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1910fe35ad75bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#training the linear svm\n",
    "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
    "#fitting the svm for bag of words\n",
    "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
    "print(svm_bow)\n",
    "#fitting the svm for tfidf features\n",
    "svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n",
    "print(svm_tfidf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70cc3541c36f8102"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model performance on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "573a1dd08e4e155"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "svm_bow_predict=svm.predict(cv_test_reviews)\n",
    "print(svm_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "svm_tfidf_predict=svm.predict(tv_test_reviews)\n",
    "print(svm_tfidf_predict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94956b02bfbcd72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce3aceff86e3a22d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "svm_bow_score=accuracy_score(test_sentiments,svm_bow_predict)\n",
    "print(\"svm_bow_score :\",svm_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "svm_tfidf_score=accuracy_score(test_sentiments,svm_tfidf_predict)\n",
    "print(\"svm_tfidf_score :\",svm_tfidf_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44cc56406adc00fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the classification report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b59aa37f2f74801"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "svm_bow_report=classification_report(test_sentiments,svm_bow_predict,target_names=['Positive','Negative'])\n",
    "print(svm_bow_report)\n",
    "#Classification report for tfidf features\n",
    "svm_tfidf_report=classification_report(test_sentiments,svm_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(svm_tfidf_report)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3761f2e5be562ad6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fedf4f25755e7da0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,svm_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,svm_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "985465f91a011029"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multinomial Naive Bayes for bag of words and tfidf features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffad3f3c5efa6099"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
    "print(mnb_bow)\n",
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
    "print(mnb_tfidf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31a0933873702088"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model performance on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194b21bb5d822044"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "300c1f4053624b6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa3da3231fd6796e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e950f2567b0e1d22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the classification report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5909568ae08c27d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)\n",
    "#Classification report for tfidf features\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25f50b15c45a6f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe9312ab0e8c3f19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5207b96f233c1ce0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word cloud for positive review words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f6e73beb7c79c6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#word cloud for positive review words\n",
    "plt.figure(figsize=(10,10))\n",
    "positive_text=norm_train_reviews[1]\n",
    "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "positive_words=WC.generate(positive_text)\n",
    "plt.imshow(positive_words,interpolation='bilinear')\n",
    "plt.show\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a628bdd244efbe66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word cloud for negative review words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2a4e5271fe68790"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Word cloud for negative review words\n",
    "plt.figure(figsize=(10,10))\n",
    "negative_text=norm_train_reviews[8]\n",
    "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "negative_words=WC.generate(negative_text)\n",
    "plt.imshow(negative_words,interpolation='bilinear')\n",
    "plt.show"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34df98f9553ef41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conclusion\n",
    "1. We can observed that both logistic regression and multinomial naive bayes model performing well compared to linear support vector machines.\n",
    "2. Still we can improve the accuracy of the models by preprocessing data and by using lexicon models like Textblob."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e251c29b4c4eb974"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
